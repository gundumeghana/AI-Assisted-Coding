{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPRh0V4q9/A818ltnFkxDHb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gundumeghana/AI-Assisted-Coding/blob/main/2403A510C1_17_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "vFAJ5n0UXayJ",
        "outputId": "120d8a4e-b5fd-4f42-fb32-fe07e5dea84e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3780803915.py:9: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df['rating'].fillna(median_rating, inplace=True)\n",
            "/tmp/ipython-input-3780803915.py:12: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
            "  df['date'] = pd.to_datetime(\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "# --- Setup and Task 1 ---\n",
        "df = pd.read_csv('customerfeedback.csv')\n",
        "\n",
        "# 1. Fill missing 'rating' values with the median\n",
        "median_rating = df['rating'].median()\n",
        "df['rating'].fillna(median_rating, inplace=True)\n",
        "\n",
        "# 2. Standardize 'date' column to YYYY-MM-DD format\n",
        "df['date'] = pd.to_datetime(\n",
        "    df['date'],\n",
        "    errors='coerce', # Set unparseable dates to NaT (which becomes 'nan' after strftime)\n",
        "    infer_datetime_format=True\n",
        ").dt.strftime('%Y-%m-%d')\n",
        "\n",
        "\n",
        "# --- Task 2: Text Cleaning ---\n",
        "\n",
        "# Define a simple spelling correction dictionary for common typos\n",
        "spell_check_dict = {\n",
        "    'amazng': 'amazing',\n",
        "    'packging': 'packaging',\n",
        "    'servce': 'service',\n",
        "    'experiance': 'experience',\n",
        "    'prodct': 'product'\n",
        "}\n",
        "\n",
        "# Hardcoded list of common English stopwords (to ensure execution without NLTK download)\n",
        "stop_words = set(['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"])\n",
        "\n",
        "def clean_text(text):\n",
        "    # a. Convert text to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # b. Correct common spelling mistakes (simplified)\n",
        "    words = text.split()\n",
        "    corrected_words = [spell_check_dict.get(re.sub(r'[^a-z]', '', word), word) for word in words]\n",
        "    text = ' '.join(corrected_words)\n",
        "\n",
        "    # c. Remove punctuation and split into words\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    words = text.split()\n",
        "\n",
        "    # d. Remove stopwords and single-character tokens\n",
        "    filtered_words = [word for word in words if word not in stop_words and len(word) > 1]\n",
        "\n",
        "    return ' '.join(filtered_words)\n",
        "\n",
        "# Apply the cleaning function\n",
        "df['cleaned_feedback'] = df['feedback_text'].apply(clean_text)\n",
        "\n",
        "# Final step: Save the processed data\n",
        "df.to_csv('customerfeedback_fully_processed.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load the Medical Records Dataset\n",
        "# NOTE: Ensure 'Medical_Records_Dataset.csv' is uploaded and accessible.\n",
        "try:\n",
        "    df_medical = pd.read_csv('Medical_Records_Dataset.csv')\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: 'Medical_Records_Dataset.csv' not found. Please upload the file.\")\n",
        "    exit()\n",
        "\n",
        "print(\"Initial data loaded. Starting preprocessing for Medical Records...\")\n",
        "\n",
        "# ==============================================================================\n",
        "#                      TASK 1: Detect and Handle Outliers in 'blood_pressure'\n",
        "# ==============================================================================\n",
        "column_name = 'blood_pressure'\n",
        "\n",
        "# 1. Calculate IQR and bounds\n",
        "Q1 = df_medical[column_name].quantile(0.25)\n",
        "Q3 = df_medical[column_name].quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "lower_bound = Q1 - 1.5 * IQR\n",
        "upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "# 2. Cap/Winsorize outliers\n",
        "# Replace values outside bounds with the respective bounds\n",
        "outliers_count = df_medical[\n",
        "    (df_medical[column_name] < lower_bound) | (df_medical[column_name] > upper_bound)\n",
        "].shape[0]\n",
        "\n",
        "df_medical[column_name] = np.where(\n",
        "    df_medical[column_name] < lower_bound,\n",
        "    lower_bound,\n",
        "    df_medical[column_name]\n",
        ")\n",
        "df_medical[column_name] = np.where(\n",
        "    df_medical[column_name] > upper_bound,\n",
        "    upper_bound,\n",
        "    df_medical[column_name]\n",
        ")\n",
        "\n",
        "print(f\"Task 1 Complete: Outliers in '{column_name}' capped (Total detected: {outliers_count}).\")\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "#                      TASK 2: Standardize and Encode 'gender'\n",
        "# ==============================================================================\n",
        "\n",
        "# 1. Standardization function to handle inconsistent labels\n",
        "def standardize_gender(gender):\n",
        "    if pd.isna(gender):\n",
        "        return 'Unknown'\n",
        "    gender = str(gender).lower().strip()\n",
        "    if gender in ['male', 'm', 'man', 'boy']:\n",
        "        return 'Male'\n",
        "    elif gender in ['female', 'f', 'woman', 'girl']:\n",
        "        return 'Female'\n",
        "    else:\n",
        "        # Catch any remaining inconsistent values\n",
        "        return 'Other'\n",
        "\n",
        "df_medical['gender_standardized'] = df_medical['gender'].apply(standardize_gender)\n",
        "\n",
        "# 2. One-Hot Encoding\n",
        "# drop_first=True is used to avoid multicollinearity by dropping the first category ('Female' if sorted, or 'Male' if the default order is used)\n",
        "df_gender_encoded = pd.get_dummies(\n",
        "    df_medical['gender_standardized'],\n",
        "    prefix='gender',\n",
        "    drop_first=True\n",
        ")\n",
        "\n",
        "# Merge the encoded columns and drop the original and intermediary columns\n",
        "df_medical = pd.concat(\n",
        "    [\n",
        "        df_medical.drop(['gender', 'gender_standardized'], axis=1),\n",
        "        df_gender_encoded\n",
        "    ],\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "print(\"Task 2 Complete: Gender standardized and One-Hot Encoded.\")\n",
        "\n",
        "# --- Final Step: Save the processed data to a new CSV file ---\n",
        "output_filename = 'medicalrecords_fully_processed.csv'\n",
        "df_medical.to_csv(output_filename, index=False)\n",
        "\n",
        "print(f\"\\n--- Preprocessing Complete ---\")\n",
        "print(f\"Processed data saved to: {output_filename}\")\n",
        "print(\"Final Data Head (showing processed 'blood_pressure' and encoded 'gender'):\")\n",
        "print(df_medical.head().to_markdown(index=False))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xyT6gf4siwx_",
        "outputId": "ef9460fc-1923-4cb5-f76e-eedca8e822f3"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial data loaded. Starting preprocessing for Medical Records...\n",
            "Task 1 Complete: Outliers in 'blood_pressure' capped (Total detected: 1).\n",
            "Task 2 Complete: Gender standardized and One-Hot Encoded.\n",
            "\n",
            "--- Preprocessing Complete ---\n",
            "Processed data saved to: medicalrecords_fully_processed.csv\n",
            "Final Data Head (showing processed 'blood_pressure' and encoded 'gender'):\n",
            "| patient_id   |   age |   blood_pressure |   cholesterol | gender_Male   |\n",
            "|:-------------|------:|-----------------:|--------------:|:--------------|\n",
            "| P001         |    25 |              120 |           180 | True          |\n",
            "| P002         |    45 |              180 |           250 | False         |\n",
            "| P003         |    36 |              130 |           200 | True          |\n",
            "| P004         |    29 |              110 |           170 | False         |\n",
            "| P005         |    67 |              215 |           300 | True          |\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# --- Setup: Load the Financial Transactions Dataset ---\n",
        "# NOTE: This script assumes a file named 'Financial_Transactions_Dataset.csv' is available.\n",
        "try:\n",
        "    df_finance = pd.read_csv('Financial_Transactions_Dataset.csv')\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: 'Financial_Transactions_Dataset.csv' not found. Please upload the file.\")\n",
        "    exit()\n",
        "\n",
        "print(\"Initial data loaded. Starting preprocessing for Financial Transactions...\")\n",
        "\n",
        "# ==============================================================================\n",
        "#                      TASK 1: Remove Duplicates and Convert Currency\n",
        "# ==============================================================================\n",
        "\n",
        "# 1. Remove duplicate transactions (keeping the first occurrence)\n",
        "initial_rows = df_finance.shape[0]\n",
        "df_finance.drop_duplicates(inplace=True)\n",
        "duplicates_removed = initial_rows - df_finance.shape[0]\n",
        "print(f\"Removed {duplicates_removed} duplicate transactions.\")\n",
        "\n",
        "# 2. Define a currency conversion dictionary (example rates relative to USD)\n",
        "CONVERSION_RATES = {\n",
        "    'USD': 1.0,\n",
        "    'EUR': 1.08,  # Example: 1 EUR = 1.08 USD\n",
        "    'INR': 0.012, # Example: 1 INR = 0.012 USD\n",
        "    'GBP': 1.25   # Example: 1 GBP = 1.25 USD\n",
        "}\n",
        "\n",
        "# 3. Conversion function to convert 'amount' to 'amount_USD'\n",
        "def convert_to_usd(row):\n",
        "    currency = str(row['currency']).upper().strip()\n",
        "    amount = row['amount']\n",
        "    # Use .get() with a default value (1.0) for safety\n",
        "    rate = CONVERSION_RATES.get(currency, 1.0)\n",
        "    return amount * rate\n",
        "\n",
        "# Apply the conversion function to create the new column\n",
        "df_finance['amount_USD'] = df_finance.apply(convert_to_usd, axis=1)\n",
        "print(\"Amounts converted to USD using the provided dictionary.\")\n",
        "\n",
        "# ==============================================================================\n",
        "#                      TASK 2: Normalize Timestamp and Extract Hour\n",
        "# ==============================================================================\n",
        "\n",
        "# 1. Normalize the 'timestamp' column to UTC\n",
        "df_finance['timestamp_utc'] = pd.to_datetime(\n",
        "    df_finance['timestamp'],\n",
        "    errors='coerce', # Handle unparseable dates by converting them to NaT\n",
        "    utc=True         # Normalize all timestamps to UTC timezone\n",
        ")\n",
        "\n",
        "# 2. Create 'transaction_hour' column (Hour of the day in UTC time, 0-23)\n",
        "df_finance['transaction_hour'] = df_finance['timestamp_utc'].dt.hour\n",
        "print(\"Timestamp normalized to UTC and 'transaction_hour' extracted.\")\n",
        "\n",
        "# 3. Drop the original 'timestamp' and 'currency' columns for a cleaner final dataset\n",
        "df_finance.drop(columns=['timestamp', 'currency'], inplace=True, errors='ignore')\n",
        "\n",
        "# --- Final Step: Save the processed data to a new CSV file ---\n",
        "output_filename = 'financialtransactions_fully_processed.csv'\n",
        "df_finance.to_csv(output_filename, index=False)\n",
        "\n",
        "print(f\"\\n--- Preprocessing Complete ---\")\n",
        "print(f\"Processed data saved to: {output_filename}\")\n",
        "print(\"Final Data Head (showing new columns):\")\n",
        "print(df_finance.head().to_markdown(index=False))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vRE8lv7phTns",
        "outputId": "9ecbb523-1e68-489d-deae-380b63115972"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial data loaded. Starting preprocessing for Financial Transactions...\n",
            "Removed 0 duplicate transactions.\n",
            "Amounts converted to USD using the provided dictionary.\n",
            "Timestamp normalized to UTC and 'transaction_hour' extracted.\n",
            "\n",
            "--- Preprocessing Complete ---\n",
            "Processed data saved to: financialtransactions_fully_processed.csv\n",
            "Final Data Head (showing new columns):\n",
            "| transaction_id   |   amount | merchant   |   amount_USD | timestamp_utc             |   transaction_hour |\n",
            "|:-----------------|---------:|:-----------|-------------:|:--------------------------|-------------------:|\n",
            "| T001             |     1000 | Amazon     |         1000 | 2025-10-01 10:00:00+00:00 |                 10 |\n",
            "| T002             |     1500 | Flipkart   |           18 | NaT                       |                nan |\n",
            "| T003             |     2000 | eBay       |         2160 | NaT                       |                nan |\n",
            "| T004             |     1000 | Amazon     |         1000 | NaT                       |                nan |\n",
            "| T005             |     2500 | Flipkart   |           30 | NaT                       |                nan |\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3103997052.py:49: FutureWarning: Parsed string \"2025-10-01 10:00:00 PST\" included an un-recognized timezone \"PST\". Dropping unrecognized timezones is deprecated; in a future version this will raise. Instead pass the string without the timezone, then use .tz_localize to convert to a recognized timezone.\n",
            "  df_finance['timestamp_utc'] = pd.to_datetime(\n"
          ]
        }
      ]
    }
  ]
}